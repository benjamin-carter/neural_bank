{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A5-churn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPf4URvCCoIN6XCS5EG72le",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjamin-carter/neural_bank/blob/master/A5_churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ_1Rf_JnHGf",
        "colab_type": "text"
      },
      "source": [
        "Connect with Google Drive in order to upload dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuXQPsvmlUjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ3_BoBbnMJm",
        "colab_type": "text"
      },
      "source": [
        "View contents of Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yse06BYSlbX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls \"/content/gdrive/My Drive/Current\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZvxrJNvnQ7-",
        "colab_type": "text"
      },
      "source": [
        "Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0efsPysliCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "from scipy.stats import multivariate_normal as mvn\n",
        "from scipy.stats import truncnorm as trn \n",
        "\n",
        "%matplotlib inline\n",
        "cmap_bold = ListedColormap([\"#FF0000\", \"#00FF00\", \"#0000FF\"])\n",
        "cmap_light = ListedColormap([\"#FFBBBB\", \"#BBFFBB\", \"#BBBBFF\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNzZ6QmwnVUQ",
        "colab_type": "text"
      },
      "source": [
        "Define and code universal functions needed for modeling and measuring effectiveness of models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NHrX-ufll_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear(H):\n",
        "  return H\n",
        "\n",
        "def ReLU(H):\n",
        "  return H * (H > 0)\n",
        "\n",
        "def sigmoid(H):\n",
        "  return 1 / (1 + np.exp(-H))\n",
        "\n",
        "def softmax(H):\n",
        "  eH = np.exp(H)\n",
        "  return eH / eH.sum(axis = 1, keepdims = True)\n",
        "\n",
        "def cross_entropy(Y, P_hat):\n",
        "  return -(1 / len(Y))*np.sum(Y * np.log(P_hat))\n",
        "\n",
        "def OLS(Y, Y_hat):\n",
        "  return (1 / (2 * len(Y))) * np.sum((Y - Y_hat) ** 2)\n",
        "\n",
        "def derivative(Z, a):\n",
        "  if a == linear: return 1\n",
        "  if a == sigmoid: return Z*(1-Z)\n",
        "  \n",
        "  elif a == np.tanh: return 1 - Z*Z \n",
        "  elif a == ReLU: return (Z > 0). astype(int)\n",
        "  else: ValueError(\"Unkown Activation\")\n",
        "\n",
        "def one_hot_encode(y):\n",
        "  N = len(y)\n",
        "  K = len(set(y))\n",
        "  Y = np.zeros((N, K))\n",
        "\n",
        "  for i in range(N):\n",
        "    Y[i,y[i]] = 1\n",
        "  return Y \n",
        "\n",
        "def accuracy(y, y_hat): \n",
        "  return np.mean(y == y_hat)\n",
        "\n",
        "def R2(y, y_hat):\n",
        "  return sqrt((y - y_hat)**2)\n",
        "\n",
        "def confusion(y, y_hat):\n",
        "  confuse = np.zeros((2,2))\n",
        "  confuse[0,0] = np.sum(y_hat[y == 1])\n",
        "  confuse[0,1] = np.sum(y_hat[y == 0])\n",
        "  confuse[1,0] = np.count_nonzero(y_hat[y == 1] == 0)\n",
        "  confuse[1,1] = np.count_nonzero(y_hat[y == 0] == 0)\n",
        "  return confuse\n",
        "\n",
        "def indices_to_one_hot(data, nb_classes):\n",
        "  #converts on iterable of indices to one hot labels\n",
        "  targets = np.array(data).reshape(-1)\n",
        "  #targets.astype(int)\n",
        "  #nb_classes.astype(int)\n",
        "  return np.eye(nb_classes)[targets]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpLbjmFzncoG",
        "colab_type": "text"
      },
      "source": [
        "Declare and code the class for teh Artificial Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7CdkMXHlosD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ANN():\n",
        "  def __init__(self, architecture, activations = None, mode = 0):\n",
        "    self.mode = mode\n",
        "    self.architecture = architecture\n",
        "    self.L = len(architecture) + 1\n",
        "    self.activations = activations\n",
        "\n",
        "  def fit(self, X, y, eta = 1e-2, epochs = 1e3, show_curve = False, lamb = 1e-2):\n",
        "    epochs = int(epochs)\n",
        "    \n",
        "    if self.mode:\n",
        "      Y = y\n",
        "    else: Y = one_hot_encode(y)\n",
        "   \n",
        "    N, D = X.shape\n",
        "    K = Y.shape[1]\n",
        "\n",
        "    #self.W = {l: np.random.randn(M[0], M[1]) for l, M in enumerate(zip(([D] + self.architecture), (self.architecture + [K])), 1)}\n",
        "    #self.b = {l: np.random.randn(M) for l, M in enumerate(self.architecture + [K], 1)}\n",
        "    \n",
        "    if self.activations is None:\n",
        "      self.a = {l: ReLU for l in range(1, self.L)}\n",
        "    else:\n",
        "      self.a = {l: act for l, act in enumerate(self.activations, 1)}\n",
        "\n",
        "    if self.mode:\n",
        "      self.a[self.L] = linear\n",
        "    else: \n",
        "      self.a[self.L] = softmax\n",
        "\n",
        "    J = np.zeros(epochs)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      self.forward(X)\n",
        "\n",
        "      if self.mode:\n",
        "        J[epoch] = OLS(Y, self.Z[self.L])\n",
        "\n",
        "      else:\n",
        "        J[epoch] = cross_entropy(Y, self.Z[self.L])\n",
        "\n",
        "      dH = (1 / N) * (self.Z[self.L] - Y)\n",
        "\n",
        "      for l in sorted(self.W.keys(), reverse = True):\n",
        "        dW = self.Z[l - 1].T @ dH\n",
        "        db = dH.sum(axis = 0)\n",
        "        self.W[l] = (1 - lamb) * self.W[l] - eta*dW\n",
        "        self.b[l] -= self.b[l] - eta*db \n",
        "        #self.W[l] = (1 - lamb) * self.W[l] - lamb*np.sign(self.W[l])- eta*dW\n",
        "        #self.W[l] -= eta*dW - lamb * self.W[l]\n",
        "        #self.b[l] -= eta*db - lamb * self.b[l]\n",
        "\n",
        "        if l > 1:\n",
        "          dZ = dH @ self.W[l].T\n",
        "          dH = dZ * derivative(self.Z[l-1], self.a[l-1])\n",
        "\n",
        "    if show_curve:\n",
        "      plt.figure()\n",
        "      plt.plot(J)\n",
        "      plt.xlabel(\"epochs\")\n",
        "      plt.ylabel(\"J\")\n",
        "      plt.title(\"Training Curve\")\n",
        "      plt.show()\n",
        "\n",
        "  def forward(self, X):\n",
        "    self.Z = {0:X}\n",
        "\n",
        "    for l in sorted(self.W.keys()):\n",
        "      self.Z[l] = self.a[l](self.Z[l-1] @ self.W[l] + self.b[l])\n",
        "\n",
        "  def predict(self, X):\n",
        "    self.forward(X)\n",
        "    \n",
        "    if self.mode:\n",
        "      return self.Z[self.L]\n",
        "\n",
        "    else:\n",
        "      return self.Z[self.L].argmax(axis = 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tca8o1utHKqp",
        "colab_type": "text"
      },
      "source": [
        "Upload data from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "typ3UEg0l-aP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "churn_df = pd.read_csv(\"/content/gdrive/My Drive/Current/churn_df.csv\")\n",
        "\n",
        "#temp = churn_df[['Exited', 'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary',\n",
        " #      'Spain', 'Germany', 'Male']]\n",
        "\n",
        "temp = churn_df[['Exited', 'HasCrCard', 'IsActiveMember', 'Spain', 'France', 'Female', 'prod1',  'Bal1', 'Bal0', 'Sal3',\n",
        "       'Sal2', 'Sal1', 'Sal4', 'CS1', 'CS0', 'Ten3', 'Ten2', 'Ten1', 'Ten0', 'Age3', 'Age1', 'Age2']]\n",
        "churn_np = temp.to_numpy()\n",
        "churn_np.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzQDFpAPECmD",
        "colab_type": "text"
      },
      "source": [
        "Run with training set as is, with Geography and Gender categorized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr1Lq_PND0yp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d844d60e-1210-44ea-a430-a4f42accd6df"
      },
      "source": [
        "N,D = churn_np.shape\n",
        "scale = .8\n",
        "test_s = int((1 - scale)*N)\n",
        "np.random.shuffle(churn_np)\n",
        "X_test = churn_np[:test_s,1:]\n",
        "y_test = churn_np[:test_s,0].astype(int)\n",
        "y_train = churn_np[test_s:,0].astype(int)\n",
        "X_train = churn_np[test_s:,1:]\n",
        "X_train.shape"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8001, 22)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GWxG4G3HIr0",
        "colab_type": "text"
      },
      "source": [
        "Normalize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK54EBjdE4aZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N, D = churn_np.shape\n",
        "\n",
        "for i in range(1,D):\n",
        "  up = np.max(churn_np[:,i])\n",
        "  down = np.min(churn_np[:,i])\n",
        "  if (up - down) != 0:\n",
        "    for j in range(N):\n",
        "      #churn_np[j,i] = (churn_np[j,i] - down) / (up - down)\n",
        "      churn_np[j,i] = 2 * (churn_np[j,i] - down) / (up - down) - 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc3vCs6cHEzQ",
        "colab_type": "text"
      },
      "source": [
        "Create training and testing sets. Also oversampling, SMOTE, and adding noise to training set as needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZtCWwhlnTzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N,D = churn_np.shape\n",
        "scale = .8\n",
        "test_s = int((1 - scale)*N)\n",
        "np.random.shuffle(churn_np)\n",
        "X_test = churn_np[:test_s,1:]\n",
        "y_test = churn_np[:test_s,0].astype(int)\n",
        "#ID_test = churn_np[:test_s, 0]\n",
        "\n",
        "X_train = churn_np[test_s:,:]\n",
        "# Oversampling\n",
        "ones_churn = X_train[X_train[:,0] == 1,:].copy()\n",
        "X_train = np.concatenate((ones_churn, X_train))\n",
        "X_train = np.concatenate((ones_churn, X_train))\n",
        "# SMOTE\n",
        "smote_churn = ones_churn.copy()\n",
        "smote_churn[:,2:] = ones_churn[:,2:] + .01 * np.random.randn()\n",
        "X_train = np.concatenate((smote_churn, X_train))\n",
        "\n",
        "\n",
        "#X_train = churn_np[test_s:,:]\n",
        "\n",
        "np.random.shuffle(X_train)\n",
        "#ID_train = X_train[:,0]\n",
        "y_train = X_train[:,0].astype(int)\n",
        "X_train = X_train[:,1:]\n",
        "# Add noise to enire training set\n",
        "X_train = X_train + .01 * np.random.randn()\n",
        "X_train.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBhOpBPmnvWL",
        "colab_type": "text"
      },
      "source": [
        "Run Artificial Neural Network with desired architecture and metrics for model verification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZWu4fW5lsxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ann = ANN([30, 25, 20, 15, 10], [np.tanh, np.tanh, np.tanh, np.tanh, np.tanh])\n",
        "ann.W = tempw\n",
        "ann.b = tempb \n",
        "ann.fit(X_train, y_train, eta = 1e-3, epochs = 1e4, show_curve = True, lamb = 0.0000)\n",
        "y_hat = ann.predict(X_test)\n",
        "y_hat_train = ann.predict(X_train)\n",
        "print(f\"Training Accuracy : {accuracy(y_train, y_hat_train):0.4f}\")\n",
        "print(f\"Testing Accuracy : {accuracy(y_test, y_hat):0.4f}\")\n",
        "tempw = ann.W\n",
        "tempb = ann.b\n",
        "print(f\"Cost : {np.sum(y_hat[y_test == 0]) + 5 * np.count_nonzero(y_hat[y_test == 1] == 0):0.4f}\")\n",
        "print(f\"1's Accuracy : {np.sum(y_hat[y_test == 1]) / (np.sum(y_hat[y_test == 1]) + np.count_nonzero(y_hat[y_test == 1] == 0)):0.4f}\")\n",
        "confusion(y_test, y_hat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbr_C7d_n_d-",
        "colab_type": "text"
      },
      "source": [
        "Save weights of the neural network when goal is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rntti5DypOEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfw = pd.DataFrame.from_dict(ann.W, orient='index')\n",
        "dfw.to_csv(\"/content/gdrive/My Drive/Current/weights_ann.csv\")\n",
        "\n",
        "dfb = pd.DataFrame.from_dict(ann.b, orient='index')\n",
        "dfb.to_csv(\"/content/gdrive/My Drive/Current/bias_ann.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgTuNajqoE6i",
        "colab_type": "text"
      },
      "source": [
        "Retrieve weights and bias of passed models as desired."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB_PKO-9NtNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfw = pd.read_csv(\"/content/gdrive/My Drive/Current/weights_ann.csv\", index_col=0, squeeze=True).to_dict()\n",
        "\n",
        "dfb = pd.read_csv(\"/content/gdrive/My Drive/Current/bias_ann.csv\", index_col=0, squeeze=True).to_dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1ZYDU_9ISlo",
        "colab_type": "text"
      },
      "source": [
        "Learning subsets of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbfRs6u04SGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "churn_df[['Exited', 'HasCrCard', 'IsActiveMember', 'Spain', 'Germany', 'France', 'Female', 'Male', 'prods4',\n",
        "          'prods3', 'prods2', 'prod1', 'Bal2', 'Bal1', 'Bal0', 'Sal4', 'Sal3', 'Sal2', 'Sal1', 'Sal0', \n",
        "          'CS3', 'CS2', 'CS1', 'CS0', 'Ten4', 'Ten3', 'Ten2', 'Ten1', 'Ten0', 'Age3', 'Age2', 'Age1', 'Age0']]\n",
        "\n",
        "#credit score\n",
        "churn_df = pd.read_csv(\"/content/gdrive/My Drive/Current/churn_df.csv\")\n",
        "credscore = churn_df[['Exited', 'HasCrCard', 'IsActiveMember', \n",
        "                      'Spain', 'Germany', 'France', 'Female', 'Male', 'prods4',\n",
        "                      'prods3', 'prods2', 'prod1', 'Bal2', 'Bal1', 'Bal0', 'Sal4', 'Sal3',\n",
        "                      'Sal2', 'Sal1', 'Sal0', 'CS3', 'CS2', 'CS1', 'CS0', 'Ten4', 'Ten3',\n",
        "                      'Ten2', 'Ten1', 'Ten0', 'Age3', 'Age2', 'Age1', 'Age0']]\n",
        "churn_df.head()\n",
        "\n",
        "N,D = churn_np.shape\n",
        "temp = churn_np[churn_np[:,-1] == 0,:]\n",
        "temp = temp[:,:-1]\n",
        "churn_np = temp\n",
        "churn_np.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eADWVcKoNN8",
        "colab_type": "text"
      },
      "source": [
        "Run General Logistic Regression model and test accuracy and given metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts6UtBHP0xpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glr = GenLogisticRegression()\n",
        "glr.fit(X_train, y_train, show_curve = True)\n",
        "y_hat_glr = glr.predict(X_test)\n",
        "y_hat_train_glr = glr.predict(X_train)\n",
        "print(f\"Training Accuracy : {accuracy(y_train, y_hat_train_glr):0.4f}\")\n",
        "print(f\"Testing Accuracy : {accuracy(y_test, y_hat_glr):0.4f}\")\n",
        "print(f\"Cost : {np.sum(y_hat_glr[y_test == 0]) + 5 * np.count_nonzero(y_hat_glr[y_test == 1] == 0):0.4f}\")\n",
        "print(f\"1's Accuracy : {np.sum(y_hat_glr[y_test == 1]) / (np.sum(y_hat_glr[y_test == 1]) + np.count_nonzero(y_hat_glr[y_test == 1] == 0)):0.4f}\")\n",
        "confusion(y_test, y_hat_glr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFuosJQ3oVkL",
        "colab_type": "text"
      },
      "source": [
        "Run KNN classifier model and test acuracy and given metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5mzDgIF1mVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "knn = KNNClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "y_hat_knn = knn.predict(X_test,15)\n",
        "y_hat_train_knn = knn.predict(X_train,15)\n",
        "print(f\"Training Accuracy : {accuracy(y_train, y_hat_train_knn):0.4f}\")\n",
        "print(f\"Testing Accuracy : {accuracy(y_test, y_hat_knn):0.4f}\")\n",
        "print(f\"Cost : {np.sum(y_hat_knn[y_test == 0]) + 5 * np.count_nonzero(y_hat_knn[y_test == 1] == 0):0.4f}\")\n",
        "print(f\"1's Accuracy : {np.sum(y_hat_knn[y_test == 1]) / (np.sum(y_hat_knn[y_test == 1]) + np.count_nonzero(y_hat_knn[y_test == 1] == 0)):0.4f}\")\n",
        "confusion(y_test, y_hat_knn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSv3ojzXoaiE",
        "colab_type": "text"
      },
      "source": [
        "Run Bayes model and test accuracy and given metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGBq2EY8BwOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run either Gauss model with original train / test data\n",
        "gnb = GaussBayes()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "y_hat_gnb = gnb.predict(X_test)\n",
        "y_hat_train_gnb = gnb.predict(X_train)\n",
        "print(f\"Training Accuracy : {accuracy(y_train, y_hat_train_gnb):0.4f}\")\n",
        "print(f\"Testing Accuracy : {accuracy(y_test, y_hat_gnb):0.4f}\")\n",
        "print(f\"Cost : {np.sum(y_hat_gnb[y_test == 0]) + 5 * np.count_nonzero(y_hat_gnb[y_test == 1] == 0):0.4f}\")\n",
        "print(f\"1's Accuracy : {np.sum(y_hat_gnb[y_test == 1]) / (np.sum(y_hat_gnb[y_test == 1]) + np.count_nonzero(y_hat_gnb[y_test == 1] == 0)):0.4f}\")\n",
        "confusion(y_test, y_hat_gnb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-lbNipSofbV",
        "colab_type": "text"
      },
      "source": [
        "Define classes for Logistic Regression, KNN Classifier, and Bayes method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvFJufG3Jv2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GenLogisticRegression():\n",
        "  \n",
        "  def __init__(self, thresh = 0.5):\n",
        "    self.W = None\n",
        "    self.B = None\n",
        "\n",
        "  def fit(self, X, y, eta = 1e-3, epochs = 1e4, show_curve = False):\n",
        "    N,D = X.shape\n",
        "    epochs = int(epochs)\n",
        "    K = len(np.unique(y))\n",
        "    \n",
        "    self.y_values = np.unique(y, return_index=False)\n",
        "    Y = indices_to_one_hot(y, K).astype(int)\n",
        "\n",
        "    self.W = np.random.randn(D, K)\n",
        "    self.B = np.random.randn(K)\n",
        "\n",
        "    J = np.zeros(int(epochs))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      P_hat = self.__forward__(X)\n",
        "      J[epoch] = cross_entropy(Y, P_hat)\n",
        "      self.W -= eta * (1/N) * X.T@(P_hat - Y)\n",
        "      self.B -= eta * (1/N) * np.sum(P_hat - Y, axis = 0)\n",
        "\n",
        "    if show_curve:\n",
        "      plt.figure()\n",
        "      plt.plot(J)\n",
        "      plt.xlabel(\"epochs\")\n",
        "      plt.ylabel(\"J\")\n",
        "      plt.title(\"Training Curve\")\n",
        "      plt.show()\n",
        "\n",
        "  def __forward__(self, X):\n",
        "    return softmax(X @ self.W + self.B)\n",
        "\n",
        "  def predict(self, X):\n",
        "    return np.argmax(self.__forward__(X),axis=1)\n",
        "\n",
        "class KNNClassifier():\n",
        "  def fit(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y.astype(int)\n",
        "  def predict(self, x, k, epsilon=1e-3):\n",
        "    N = len(x)\n",
        "    y_hat = np.zeros(N)\n",
        "    for i in range(N):\n",
        "      dist2 = np.sum((self.x - x[i])**2, axis =1)\n",
        "      idxt = np.argsort(dist2)[:k]\n",
        "      gamma_k = (np.sqrt(dist2[idxt] + epsilon))**-1\n",
        "      y_hat[i] = np.bincount(self.y[idxt], weights = gamma_k).argmax()\n",
        "    return y_hat\n",
        "\n",
        "## Bayes with Gaussian distribution\n",
        "class GaussBayes():\n",
        "  # fitting model with training data. input data split into features, X, and labels, y\n",
        "  def fit(self, X, y, epsilon = 1e-3):\n",
        "    self.likelihoods = dict()\n",
        "    self.priors = dict()\n",
        "    self.K = set(y.astype(int))\n",
        "    for k in self.K:\n",
        "      X_k = X[y == k, :]\n",
        "      N_k, D = X_k.shape\n",
        "      self.likelihoods[k] = {\"Mean\": X_k.mean(axis=0), \"Cov\": (1/(N_k - 1)) * np.matmul((X_k - X_k.mean(axis=0)).T, X_k - X_k.mean(axis=0)) \n",
        "      + epsilon*np.identity(D)}\n",
        "      self.priors[k] = len(X_k) / len(X)\n",
        "  # prediction function with test data. input is only features.\n",
        "  def predict(self, X):\n",
        "    N, D = X.shape\n",
        "    P_hat = np.zeros((N, len(self.K)))\n",
        "    for k, l in self.likelihoods.items():\n",
        "      P_hat[:, k] = mvn.logpdf(X, l[\"Mean\"], l[\"Cov\"]) + np.log(self.priors[k])  \n",
        "    return P_hat.argmax(axis=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6DT_waNosrC",
        "colab_type": "text"
      },
      "source": [
        "Run varying models for subsets of data based on categorical variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "77lCTKuFvlLR",
        "colab": {}
      },
      "source": [
        "#churn_df[['Exited', 'HasCrCard', 'IsActiveMember', 'Spain', 'Germany', 'France', 'Female', 'Male', 'prods4',\n",
        "#          'prods3', 'prods2', 'prod1', 'Bal2', 'Bal1', 'Bal0', 'Sal4', 'Sal3', 'Sal2', 'Sal1', 'Sal0', \n",
        "#          'CS3', 'CS2', 'CS1', 'CS0', 'Ten4', 'Ten3', 'Ten2', 'Ten1', 'Ten0', 'Age3', 'Age2', 'Age1', 'Age0']]\n",
        "\n",
        "#credit score\n",
        "churn_df = pd.read_csv(\"/content/gdrive/My Drive/Current/churn_df.csv\")\n",
        "german = churn_df[['Exited', 'HasCrCard', 'IsActiveMember', 'Germany', 'France', 'Male', 'prods2', 'Bal1', 'Bal0', \n",
        "                    'Sal3', 'Sal2', 'Sal1', 'Sal4', 'CS2', 'CS1', 'CS0', 'Ten3', 'Ten2', 'Ten1', 'Ten0',  'Age3']]  \n",
        "churn_np = german.to_numpy()\n",
        "\n",
        "N,D = churn_np.shape\n",
        "temp = churn_np[churn_np[:,-1] == 1,:]\n",
        "temp = temp[:,:-1]\n",
        "churn_np = temp\n",
        "print(churn_np.shape)\n",
        "N,D = churn_np.shape\n",
        "for i in range(1,D):\n",
        "  up = np.max(churn_np[:,i])\n",
        "  down = np.min(churn_np[:,i])\n",
        "  if (up - down) != 0:\n",
        "    for j in range(N):\n",
        "      #churn_np[j,i] = (churn_np[j,i] - down) / (up - down)\n",
        "      churn_np[j,i] = 2 * (churn_np[j,i] - down) / (up - down) - 1\n",
        "\n",
        "scale = .8\n",
        "test_s = int((1 - scale)*N)\n",
        "np.random.shuffle(churn_np)\n",
        "X_test = churn_np[:test_s,1:]\n",
        "y_test = churn_np[:test_s,0].astype(int)\n",
        "#ID_test = churn_np[:test_s, 0]\n",
        "\n",
        "X_train = churn_np[test_s:,:]\n",
        "# Oversampling\n",
        "ones_churn = X_train[X_train[:,0] == 1,:].copy()\n",
        "X_train = np.concatenate((ones_churn, X_train))\n",
        "X_train = np.concatenate((ones_churn, X_train))\n",
        "\n",
        "np.random.shuffle(X_train)\n",
        "#ID_train = X_train[:,0]\n",
        "y_train = X_train[:,0].astype(int)\n",
        "X_train = X_train[:,1:]\n",
        "# Add noise to enire training set\n",
        "X_train = X_train + .01 * np.random.randn()\n",
        "X_train.shape\n",
        "\n",
        "\n",
        "ann = ANN([35, 25, 20, 15, 10], [np.tanh, np.tanh, np.tanh, np.tanh, np.tanh])\n",
        "#ann.W = tempw\n",
        "#ann.b = tempb \n",
        "ann.fit(X_train, y_train, eta = 1e-2, epochs = 1e3, show_curve = True, lamb = 0.000)\n",
        "y_hat = ann.predict(X_test)\n",
        "y_hat_train = ann.predict(X_train)\n",
        "print(f\"Training Accuracy : {accuracy(y_train, y_hat_train):0.4f}\")\n",
        "print(f\"Testing Accuracy : {accuracy(y_test, y_hat):0.4f}\")\n",
        "tempw = ann.W\n",
        "tempb = ann.b\n",
        "print(f\"Cost : {np.sum(y_hat[y_test == 0]) + 5 * np.count_nonzero(y_hat[y_test == 1] == 0):0.4f}\")\n",
        "print(f\"1's Accuracy : {np.sum(y_hat[y_test == 1]) / (np.sum(y_hat[y_test == 1]) + np.count_nonzero(y_hat[y_test == 1] == 0)):0.4f}\")\n",
        "confusion(y_test, y_hat)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}